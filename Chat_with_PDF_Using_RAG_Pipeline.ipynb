{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tittle : Chat with PDF Using RAG Pipeline**"
      ],
      "metadata": {
        "id": "RQev6gAVMFxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview**\n",
        "\n",
        "\"Imagine having a large PDF document and being able to instantly get answers to your questions without manually searching through pages. In this project, I’ve built a RAG pipeline that allows you to upload or fetch a PDF, ask a question about its content, and receive an accurate response. This system combines advanced Natural Language Processing techniques and tools like Hugging Face models, FAISS, and Sentence Transformers.\""
      ],
      "metadata": {
        "id": "twGYb1ibL7bj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxnwFE-WehlE",
        "outputId": "d016a0df-b14f-41ce-c0e9-db9f0e5ab34a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install requests\n",
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import faiss\n",
        "\n",
        "# Initialize the Hugging Face model for response generation\n",
        "model_name = 't5-small'  # You can also use 'facebook/bart-large' for BART\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the sentence transformer model for embeddings\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#Explanation:\n",
        "#1.The SentenceTransformer model (all-MiniLM-L6-v2) is used to generate embeddings for textual data.\n",
        "#2.This is a lightweight transformer-based model optimized for semantic similarity tasks.\n",
        "\n",
        "# Function to download the PDF from URL\n",
        "def download_pdf(pdf_url, download_path='/content/temp_pdf.pdf'):\n",
        "    response = requests.get(pdf_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(download_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        return download_path\n",
        "    else:\n",
        "        return None\n",
        "#Explanation:\n",
        "#We use the requests library to download the PDF file from a provided URL.\n",
        "\n",
        "# Function to extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(pdf_document.page_count):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        text += page.get_text(\"text\")\n",
        "    return text\n",
        "\n",
        "#The tool PyMuPDF extracts textual data from the PDF. It supports multi-page PDFs and outputs clean text.\n",
        "\n",
        "# Function to chunk the text\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    chunks = []\n",
        "    words = text.split()\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "#Explanation:\n",
        "#1.Splits the extracted text into smaller chunks (default size: 500 words).\n",
        "#2.These smaller chunks ensure that the embeddings are more manageable and relevant for semantic search.\n",
        "\n",
        "# Function to create embeddings from text chunks\n",
        "def create_embeddings(chunks):\n",
        "    return embedding_model.encode(chunks)\n",
        "\n",
        "#Generates vector embeddings for a list of text chunks using the SentenceTransformer model.\n",
        "#The SentenceTransformer model converts each text chunk into a dense vector representation of fixed dimensions\n",
        "\n",
        "# Function to store embeddings in FAISS (indexing and search)\n",
        "def store_embeddings_in_faiss(embeddings):\n",
        "    dimension = embeddings.shape[1]  # Dimension of embeddings\n",
        "    index = faiss.IndexFlatL2(dimension)  # Using L2 distance for similarity search\n",
        "    index.add(embeddings)  # Add embeddings to the index\n",
        "    return index\n",
        "\n",
        "#Stores the embeddings into a FAISS index to enable fast and efficient similarity search.\n",
        "\"works by Determines the dimensionality of the embeddings, Creates a FAISS index that uses L2 distance for similarity computation\"\n",
        "\"Adds all the embeddings to the FAISS index for efficient retrieval.\"\n",
        "\n",
        "# Function to handle user query and get the most relevant chunks\n",
        "def handle_query(query, index, chunks, top_k=3):\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "\n",
        "    # Search for the top K most relevant chunks based on the query embedding\n",
        "    D, I = index.search(query_embedding, top_k)  # D: distances, I: indices of the retrieved chunks\n",
        "\n",
        "    # Retrieve the corresponding chunks\n",
        "    relevant_chunks = [chunks[i] for i in I[0]]\n",
        "    return relevant_chunks\n",
        "\"When the user inputs a query, we embed the query, search the FAISS index for the closest matching top k chunks , and generate a response.\"\n",
        "\n",
        "# Function to generate response using Hugging Face T5\n",
        "def generate_response(query, relevant_chunks):\n",
        "    context = \"\\n\".join(relevant_chunks)  # Concatenate the retrieved chunks\n",
        "\n",
        "    # Prepare the input prompt for the model\n",
        "    input_text = f\"Question: {query}\\nContext: {context}\\nAnswer:\"\n",
        "\n",
        "    # Tokenize the input and generate the output\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs, max_length=150, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the output sequence\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\"The T5 model takes the question and context as input to generate the final answer.\"\n",
        "#Combines the most relevant text chunks into a context and formulates a response.\n",
        "#The response structure includes:\n",
        "#The user’s query.\n",
        "#The most relevant chunks from the PDF content.\n",
        "\n",
        "# Complete pipeline: from PDF processing to query handling\n",
        "def run_pipeline(pdf_url, user_query):\n",
        "    # Download and process the PDF\n",
        "    pdf_path = download_pdf(pdf_url)\n",
        "    if pdf_path:\n",
        "        # Extract text and chunk it\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        chunks = chunk_text(text)\n",
        "\n",
        "        # Generate embeddings and store in FAISS\n",
        "        embeddings = create_embeddings(chunks)\n",
        "        embeddings = np.array(embeddings)\n",
        "        index = store_embeddings_in_faiss(embeddings)\n",
        "\n",
        "        # Handle the user query\n",
        "        relevant_chunks = handle_query(user_query, index, chunks)\n",
        "\n",
        "        # Generate the final response using Hugging Face model\n",
        "        response = generate_response(user_query, relevant_chunks)\n",
        "        return response\n",
        "    else:\n",
        "        return \"Failed to download the PDF from the provided URL.\"\n",
        "#Explanation:\n",
        "#Function: run_pipeline(pdf_path, user_query)\n",
        "#1.Combines all steps into a single pipeline:\n",
        "#2.Text Extraction: Extract text from the given PDF (URL or local).\n",
        "#3.Text Chunking: Split the text into manageable pieces.\n",
        "#4.Embedding Creation: Generate and/or load cached embeddings.\n",
        "#5.Index Storage: Store embeddings in FAISS for efficient retrieval.\n",
        "#6.Similarity Search: Retrieve the most relevant chunks for the user query.\n",
        "#7.Response Generation: Generate a meaningful response based on the relevant chunks.\n"
      ],
      "metadata": {
        "id": "dBeXYuqkenzn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "pdf_url = \"https://www.hunter.cuny.edu/dolciani/pdf_files/workshop-materials/mmc-presentations/tables-charts-and-graphs-with-examples-from.pdf\"\n",
        "user_query = \"what is overview of this pdf?\"\n",
        "\n",
        "response = run_pipeline(pdf_url, user_query)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXDQie1XgUwR",
        "outputId": "7c1582c2-e7dc-4fc0-f7dc-a3fc90aa09b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life REVISED: MICHAEL LOLKUS 2018 Tables, Charts, and Graphs Basics We use charts and graphs to visualize data. This data can either be generated data, data gathered from an experiment, or data collected from some source. Notice the pie chart below is not very intuitive. Example from Everyday Life 19% 10% 10% 15% 5% 26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YldnS079NAl9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}