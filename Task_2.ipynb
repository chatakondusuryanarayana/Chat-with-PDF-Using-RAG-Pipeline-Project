{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers faiss-cpu beautifulsoup4 requests\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y0WRafP1ez1j",
        "outputId": "cd9e8efa-f124-4386-fc9d-007af2b245e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting faiss-cpu\n",
            "  Using cached faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PtZFnXkdRN1",
        "outputId": "976cad2e-1661-4e8b-82ce-dd7b169b10a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            " Context:\n",
            "- © 2024 University of North Dakota - Grand Forks, ND - Member of ND University System\n",
            "- More than two-thirds of undergrads receive some form of financial assistance. Generally, tuition is covered for families with incomes below $150,000.\n",
            "- The University of North Dakota values, honors, and supports all members of our campus\n",
            "                                    community.\n",
            "- Online learning option is perfect for student living in western North Dakota.\n",
            "- The University of North Dakota is the state's oldest and largest university. We offer\n",
            "                           225+ highly accredited on-campus and online degrees.\n",
            "\n",
            "Question: How can students apply for financial aid at the University of North Dakota?\n",
            "\n",
            "Answer:\n",
            "Students who are enrolled in a degree program at the University of North Dakota may apply for financial aid at the University of North Dakota. The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Students who are enrolled in a degree program at the University of North Dakota may apply for financial aid at the University of North Dakota. The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Students who are enrolled in a degree program at the University of North Dakota may apply for financial aid at the University of North Dakota. The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Students who\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# Initialize Models\n",
        "# SentenceTransformer for generating embeddings from text\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
        "# BLOOM tokenizer and model for generating answers to user queries\n",
        "bloom_tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
        "bloom_model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
        "\n",
        "# FAISS Vector Store setup\n",
        "dimension = 384  # The embedding dimension size of the model (for FAISS index)\n",
        "index = faiss.IndexFlatL2(dimension)  # FAISS index to store vector embeddings and perform efficient similarity search\n",
        "documents = []  # List to store the raw document text along with metadata\n",
        "\n",
        "# Function to crawl and scrape websites\n",
        "def crawl_website(url):\n",
        "    # Send HTTP request to fetch content of the webpage\n",
        "    response = requests.get(url)\n",
        "    # Parse the page content using BeautifulSoup to extract the textual data\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    # Extract text from all paragraphs and remove empty ones\n",
        "    paragraphs = [p.text.strip() for p in soup.find_all('p') if p.text.strip()]\n",
        "    return paragraphs\n",
        "\n",
        "# Function to generate embeddings and add them to FAISS index\n",
        "def add_to_faiss(texts, url):\n",
        "    global documents\n",
        "    # Convert each text into vector embeddings using the pre-trained embedding model\n",
        "    embeddings = embedding_model.encode(texts, convert_to_numpy=True)\n",
        "    # Add embeddings to FAISS index\n",
        "    index.add(embeddings)\n",
        "    # Store the text and URL metadata in the documents list for later retrieval\n",
        "    documents.extend([{\"text\": text, \"url\": url} for text in texts])\n",
        "\n",
        "# Function to retrieve similar documents based on the query\n",
        "def retrieve_documents(query, top_k=5):\n",
        "    # Convert the user's query into embeddings\n",
        "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
        "    # Perform similarity search in FAISS index to retrieve top_k most similar text chunks\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    # Collect the actual documents that correspond to the indices found by FAISS\n",
        "    retrieved = [documents[idx] for idx in indices[0] if idx < len(documents)]\n",
        "    return retrieved\n",
        "\n",
        "# Function to generate a response using the BLOOM model\n",
        "def generate_response(query, retrieved_docs):\n",
        "    # Prepare context for the model by joining retrieved documents into a single string\n",
        "    context = \"\\n\".join([f\"- {doc['text']}\" for doc in retrieved_docs])\n",
        "    # Create a prompt that includes the context and the query for the LLM to generate an answer\n",
        "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    # Tokenize the prompt to feed into the BLOOM model\n",
        "    inputs = bloom_tokenizer(prompt, return_tensors=\"pt\")\n",
        "    # Generate a response from the model, limiting the response to 300 tokens\n",
        "    outputs = bloom_model.generate(**inputs, max_length=300, num_return_sequences=1)\n",
        "    # Decode the model's response into a human-readable format\n",
        "    return bloom_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Main function to implement the RAG pipeline\n",
        "def rag_pipeline(query, urls):\n",
        "    # Step 1: Crawl and process websites, scraping the textual content from each URL\n",
        "    for url in urls:\n",
        "        paragraphs = crawl_website(url)\n",
        "        add_to_faiss(paragraphs, url)\n",
        "\n",
        "    # Step 2: Retrieve the most relevant documents based on the user’s query\n",
        "    retrieved_docs = retrieve_documents(query)\n",
        "\n",
        "    # Step 3: Generate a response using the retrieved documents\n",
        "    response = generate_response(query, retrieved_docs)\n",
        "    return response\n",
        "\n",
        "# Example usage of the RAG pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Define a list of websites to scrape\n",
        "    urls = [\n",
        "        \"https://www.uchicago.edu/\",\n",
        "        \"https://www.stanford.edu/\",\n",
        "        \"https://www.washington.edu/\",\n",
        "        \"https://und.edu/\"\n",
        "    ]\n",
        "    # Define a sample user query\n",
        "    user_query = \"How can students apply for financial aid at the University of North Dakota?\"\n",
        "\n",
        "    # Run the RAG pipeline to generate an answer based on the provided query and websites\n",
        "    response = rag_pipeline(user_query, urls)\n",
        "    # Print the generated response\n",
        "    print(\"Response:\\n\", response)"
      ]
    }
  ]
}